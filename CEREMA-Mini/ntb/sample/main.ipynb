{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SETUP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs && Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import traci\n",
    "import sumolib\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyproj import Transformer, CRS\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.errors import TopologicalError\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "# export LD_LIBRARY_PATH=~/Libs/libnsl\n",
    "# export SUMO_HOME=~/Envs/sumo-env/lib/python3.10/site-packages/sumo\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.expanduser(\"~/Libs/libnsl\")\n",
    "os.environ[\"SUMO_HOME\"] = os.path.expanduser(\"~/Envs/sumo-env/lib/python3.10/site-packages/sumo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed PATHs\n",
    "NET_XML = Path(\"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/data/newtest-osm.net.xml\")\n",
    "POLY_XML = \"/home/hoai-linh.dao/Works/EVCS/AMP-Metropole/Task-1-Completion/results/p0/newtest-poly/bassin-based.poly.xml\"\n",
    "ORIG_VTYPES_XML = \"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/data/integrated-dist.add.xml\"\n",
    "GROUPED_POLY_XML = \"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/data/group-based.poly.xml\"\n",
    "FLOW_CSV = \"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/data/flow.csv\"\n",
    "MAIN_FLOW_CSV = \"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/result/main-flow.csv\"\n",
    "\n",
    "SUMO_TOOLS_DIR = Path(\"/home/hoai-linh.dao/Envs/sumo-env/lib/python3.10/site-packages/sumo/tools\")\n",
    "REROUTING_PY = SUMO_TOOLS_DIR / \"generateContinuousRerouters.py\"\n",
    "NETCHECK_PY = SUMO_TOOLS_DIR / \"net/netcheck.py\"\n",
    "RANDOMTRIPS_PY = SUMO_TOOLS_DIR / \"randomTrips.py\"\n",
    "FINDALLROUTES_PY = SUMO_TOOLS_DIR / \"findAllRoutes.py\"\n",
    "PLOTXMLATTRIBUTES_PY = SUMO_TOOLS_DIR / \"visualization/plotXMLAttributes.py\"\n",
    "PLOTTRAJECTORIES_PY = SUMO_TOOLS_DIR / \"plot_trajectories.py\"\n",
    "PLOTNETDUMP_PY = SUMO_TOOLS_DIR / \"visualization/plot_net_dump.py\"\n",
    "PLOTNETSPEED_PY = SUMO_TOOLS_DIR / \"visualization/plot_net_speed.py\"\n",
    "PLOTNETTRAFFICLIGHTS_PY = SUMO_TOOLS_DIR / \"visualization/plot_net_trafficLights.py\"\n",
    "PLOTSUMMARY_PY = SUMO_TOOLS_DIR / \"visualization/plot_summary.py\"\n",
    "PLOTTRIPINFODISTRIBUTIONS_PY = SUMO_TOOLS_DIR / \"visualization/plot_tripinfo_distributions.py\"\n",
    "PLOTCSVTIMELINE_PY = SUMO_TOOLS_DIR / \"visualization/plot_csv_timeline.py\"\n",
    "PLOTCSVPIE_PY = SUMO_TOOLS_DIR / \"visualization/plot_csv_pie.py\"\n",
    "PLOTCSVBARS_PY = SUMO_TOOLS_DIR / \"visualization/plot_csv_bars.py\"\n",
    "MACROUTPUT_PY = SUMO_TOOLS_DIR / \"visualization/marcoOutput.py\"\n",
    "ROUTESTATS_PY = SUMO_TOOLS_DIR / \"route/routeStats.py\"\n",
    "ROUTECHECK_PY = SUMO_TOOLS_DIR / \"route/routecheck.py\"\n",
    "\n",
    "# Dynamic DIRs\n",
    "SIMULATION_DIR = Path(\"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/result/experiments/simulation-debug-1%\")\n",
    "\n",
    "ODS_DIR = SIMULATION_DIR / \"ods\"\n",
    "TRIPS_DIR = SIMULATION_DIR / \"trips\"\n",
    "OUTPUTS_DIR = SIMULATION_DIR / \"outputs\"\n",
    "LOGS_DIR = SIMULATION_DIR / \"logs\"\n",
    "VISUALIZATIONS_DIR = SIMULATION_DIR / \"visualizations\"\n",
    "\n",
    "SIMULATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for path in [ODS_DIR, TRIPS_DIR, OUTPUTS_DIR, LOGS_DIR, VISUALIZATIONS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dynamic PATHs\n",
    "TAZ_XML = SIMULATION_DIR / \"taz.add.xml\"\n",
    "VTYPES_DIST_XML = SIMULATION_DIR / \"vtypes-dist.add.xml\"\n",
    "ALL_TRIPS_XML = SIMULATION_DIR / \"trips.xml\"\n",
    "ROUTE_XML = SIMULATION_DIR / \"route.xml\"\n",
    "ROUTE_ALT_XML = SIMULATION_DIR / \"route.alt.xml\"\n",
    "REROUTER_XML = SIMULATION_DIR / \"rerouter.add.xml\"\n",
    "SUMOCFG_XML = SIMULATION_DIR / \"run.sumocfg\"\n",
    "\n",
    "DUAROUTER_LOG = LOGS_DIR / \"duarouter.log\"\n",
    "SIMULATION_LOG = LOGS_DIR / \"sumo_run.log\"\n",
    "REROUTING_LOG = LOGS_DIR / \"rerouting.log\"\n",
    "\n",
    "# Outputs Paths\n",
    "COLLISIONS_XML = OUTPUTS_DIR / \"collisions.xml\"\n",
    "BATTERY_XML = OUTPUTS_DIR / \"battery.xml\"\n",
    "LANECHANGES_XML = OUTPUTS_DIR / \"laneChanges.xml\"\n",
    "STATISTICS_XML = OUTPUTS_DIR / \"statistics.xml\"\n",
    "TRACE_XML = OUTPUTS_DIR / \"sumoTrace.xml\"\n",
    "SUMMARY_XML = OUTPUTS_DIR / \"summary.xml\"\n",
    "TRIPINFO_XML = OUTPUTS_DIR / \"tripinfo.xml\"\n",
    "VEHROUTES_XML = OUTPUTS_DIR / \"vehRoutes.xml\"\n",
    "NETSTATE_XML = OUTPUTS_DIR / \"netstate.xml\"\n",
    "LOG_TXT = OUTPUTS_DIR / \"log.txt\"\n",
    "\n",
    "# Visualization Paths\n",
    "PLOT_1_PNG = VISUALIZATIONS_DIR / \"plot_1.png\"\n",
    "PLOT_2_PNG = VISUALIZATIONS_DIR / \"plot_2.png\"\n",
    "PLOT_3_PNG = VISUALIZATIONS_DIR / \"plot_3.png\"\n",
    "PLOT_4_PNG = VISUALIZATIONS_DIR / \"plot_4.png\"\n",
    "PLOT_5_PNG = VISUALIZATIONS_DIR / \"plot_5.png\"\n",
    "PLOT_6_PNG = VISUALIZATIONS_DIR / \"plot_6.png\"\n",
    "PLOT_7_PNG = VISUALIZATIONS_DIR / \"plot_7.png\"\n",
    "PLOT_8_PNG = VISUALIZATIONS_DIR / \"plot_8.png\"\n",
    "\n",
    "# Net-Repairment Task\n",
    "NET_REPAIRMENT_DIR = Path(\"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/result/net-repairment\")\n",
    "CLEANED_NET_XML_1 = NET_REPAIRMENT_DIR /  f\"cleaned_1_{NET_XML.name}\"\n",
    "CLEANED_NET_XML_2 = NET_REPAIRMENT_DIR /  f\"cleaned_2_{NET_XML.name}\"\n",
    "\n",
    "KEEP_EDGES_TXT_1 = NET_REPAIRMENT_DIR / \"keep-edges_1.txt\"\n",
    "KEEP_EDGES_TXT_2 = NET_REPAIRMENT_DIR / \"keep-edges_2.txt\"\n",
    "COMPONENTS_NW_TXT_1 = NET_REPAIRMENT_DIR / \"components_nw_1.txt\"\n",
    "COMPONENTS_NW_TXT_2 = NET_REPAIRMENT_DIR / \"components_nw_2.txt\"\n",
    "\n",
    "NET_REPAIRMENT_LOGS_DIR = NET_REPAIRMENT_DIR / \"logs\"\n",
    "NETCHECK_LOG_1 = NET_REPAIRMENT_LOGS_DIR / \"netcheck_1.log\"\n",
    "NETCHECK_LOG_2 = NET_REPAIRMENT_LOGS_DIR / \"netcheck_2.log\"\n",
    "NETCHECK_LOG_3 = NET_REPAIRMENT_LOGS_DIR / \"netcheck_3.log\"\n",
    "NETCHECK_LOG_4 = NET_REPAIRMENT_LOGS_DIR / \"netcheck_4.log\"\n",
    "\n",
    "NETCONVERT_LOG_1 = NET_REPAIRMENT_LOGS_DIR / \"netconvert_1.log\"\n",
    "NETCONVERT_LOG_2 = NET_REPAIRMENT_LOGS_DIR / \"netconvert_2.log\"\n",
    "NETCONVERT_LOG_3 = NET_REPAIRMENT_LOGS_DIR / \"netconvert_3.log\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAZ_IDS = {\n",
    "    'marseille': '1',\n",
    "    'aix-en-provence': '2',\n",
    "    'est-etang-de-berre': '3',\n",
    "    'nord-ouest': '4',\n",
    "    'ouest-etang-de-berre': '5',\n",
    "    'sud-est': '6',\n",
    "    'hors_amp': '99'\n",
    "}\n",
    "\n",
    "BORDER_RATIO = 0.40\n",
    "REAL_ORIGIN   = 'marseille'\n",
    "\n",
    "CAR_PREFIX = \"carDist\"              \n",
    "EV_BRANDS = [\"Renault\", \"Tesla\", \"Citroen\", \"Peugeot\", \"Dacia\", \"Volkswagen\", \"BMW\", \"Fiat\", \"KIA\"]\n",
    "\n",
    "EV_RATIO = 0.20\n",
    "\n",
    "DIST_ID = \"vehDist\"\n",
    "\n",
    "# Page 11\n",
    "INCOMING_RATIO = 178729/(178729 + 174729)\n",
    "OUTGOING_RATIO = 174729/(178729 + 174729)\n",
    "INCOMING_RATIO, OUTGOING_RATIO\n",
    "\n",
    "# Page 14 + Page 15\n",
    "TRIPS_RATIO_0 = 1 # default\n",
    "TRIPS_RATIO_1 = 0.40 # Marseille \n",
    "TRIPS_RATIO_2 = 0.41 # Marseille Bassin\n",
    "TRIPS_RATIO_3 = 0.52 # AMP Bassin = CEREMA\n",
    "TRIPS_RATIO_4 = 0.10 # test\n",
    "TRIPS_RATIO_5 = 0.01 # fast test\n",
    "\n",
    "PATH_REPLACEMENTS = {\n",
    "    'net-file': CLEANED_NET_XML_2,\n",
    "    'route-files': ROUTE_XML,\n",
    "    'summary-output': SUMMARY_XML,\n",
    "    'tripinfo-output': TRIPINFO_XML,\n",
    "    'fcd-output': TRACE_XML,\n",
    "    'lanechange-output': LANECHANGES_XML,\n",
    "    'battery-output': BATTERY_XML,\n",
    "    'vehroute-output': VEHROUTES_XML,\n",
    "    'collision-output': COLLISIONS_XML,\n",
    "    'netstate-dump': NETSTATE_XML,\n",
    "    'statistic-output': STATISTICS_XML,\n",
    "    'log': LOG_TXT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Raw Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FLOW_CSV)\n",
    "\n",
    "columns_inter = ['Est_Etang-de-Berre','Aix-en-Provence','Sud-Est','Ouest_Etang-de-Berre','Nord-Ouest','Hors_AMP']\n",
    "df[\"Intra\"] = df[\"Total\"] - df[columns_inter].sum(axis=1)\n",
    "\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "df.to_csv(MAIN_FLOW_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repairing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCHECK_CMD_1 = [\n",
    "    \"python\", NETCHECK_PY,\n",
    "    NET_XML,\n",
    "    \"--vclass\", \"passenger\",\n",
    "    \"--component-output\", COMPONENTS_NW_TXT_1\n",
    "]\n",
    "\n",
    "with open(NETCHECK_LOG_1, \"w\") as f:\n",
    "    print(f\"Running NETCHECK Step 1 ...\")\n",
    "    subprocess.run(NETCHECK_CMD_1, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Components Ouput written to {COMPONENTS_NW_TXT_1}\\n[LOG] Output logged in {NETCHECK_LOG_1}\")\n",
    "\n",
    "print()\n",
    "print(f\"Running extractMaxComponent ...\")\n",
    "extractMaxComponent(COMPONENTS_NW_TXT_1, KEEP_EDGES_TXT_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCONVERT_CMD_1 = [\n",
    "    \"netconvert\",\n",
    "    \"--net-file\", NET_XML,\n",
    "    \"--keep-edges.input-file\", KEEP_EDGES_TXT_1,\n",
    "    \"--geometry.remove\",\n",
    "    \"--geometry.remove.min-length\", \"2\",\n",
    "    \"--geometry.max-segment-length\", \"20\",\n",
    "    \"--geometry.min-dist\", \"0.1\",\n",
    "    \"--geometry.max-angle\", \"150\",\n",
    "    \"--geometry.max-angle.fix\",\n",
    "    \"--remove-edges.isolated\",\n",
    "    \"--junctions.join\",\n",
    "    \"--junctions.join-dist\", \"60\",\n",
    "    \"--roundabouts.guess\",\n",
    "    \"--ramps.guess\",\n",
    "    \"--keep-edges.by-vclass=passenger\",\n",
    "    \"--osm.bike-access=false\",\n",
    "    \"--osm.sidewalks=false\",\n",
    "    \"--crossings.guess=false\",\n",
    "    \"--tls.guess\",\n",
    "    \"--tls.guess.threshold\", \"40\",\n",
    "    \"--tls.join\",\n",
    "    \"--tls.layout\", \"incoming\",\n",
    "    \"--tls.discard-loaded\",\n",
    "    \"--ptstop-output\", \"/dev/null\",\n",
    "    \"--ptline-output\", \"/dev/null\",\n",
    "    \"-o\", CLEANED_NET_XML_1\n",
    "]\n",
    "\n",
    "with open(NETCONVERT_LOG_1, \"w\") as f:\n",
    "    print(f\"Running NETCONVERT Step 1 ...\")\n",
    "    subprocess.run(NETCONVERT_CMD_1, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Cleaned Network written to {CLEANED_NET_XML_1}\\n[LOG] Output logged in {NETCONVERT_LOG_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCHECK_CMD_2 = [\n",
    "    \"python\", NETCHECK_PY,\n",
    "    CLEANED_NET_XML_1,\n",
    "    \"--vclass\", \"passenger\",\n",
    "    \"--component-output\", COMPONENTS_NW_TXT_2,\n",
    "    \"-t\"\n",
    "]\n",
    "\n",
    "with open(NETCHECK_LOG_2, \"w\") as f:\n",
    "    print(f\"Running NETCHECK Step 2 ...\")\n",
    "    subprocess.run(NETCHECK_CMD_2, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Output logged in {NETCHECK_LOG_2}\")\n",
    "\n",
    "print()\n",
    "print(f\"Running extractMaxComponent ...\")\n",
    "extractMaxComponent(COMPONENTS_NW_TXT_2, KEEP_EDGES_TXT_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCONVERT_CMD_2 = [\n",
    "    \"netconvert\",\n",
    "    \"--net-file\", CLEANED_NET_XML_1,\n",
    "    \"--keep-edges.input-file\", KEEP_EDGES_TXT_2,\n",
    "    \"-o\", CLEANED_NET_XML_2\n",
    "]\n",
    "\n",
    "with open(NETCONVERT_LOG_2, \"w\") as f:\n",
    "    print(f\"Running NETCONVERT Step 2 ...\")\n",
    "    subprocess.run(NETCONVERT_CMD_2, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Cleaned Network written to {CLEANED_NET_XML_2}\\n[LOG] Output logged in {NETCONVERT_LOG_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCHECK_CMD_3 = [\n",
    "    \"python\", NETCHECK_PY,\n",
    "    CLEANED_NET_XML_2,\n",
    "    \"--vclass\", \"passenger\",\n",
    "    \"-t\"\n",
    "\n",
    "]\n",
    "\n",
    "with open(NETCHECK_LOG_3, \"w\") as f:\n",
    "    print(f\"Running NETCHECK Step 3 ...\")\n",
    "    subprocess.run(NETCHECK_CMD_3, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Output logged in {NETCHECK_LOG_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCONVERT_CMD_3 = [\n",
    "    \"netconvert\",\n",
    "    \"--net-file\", CLEANED_NET_XML_2,          # đầu vào: mạng sau bước 2\n",
    "    \"--geometry.min-radius.fix\", \"10\",        # làm mượt cua gấp (<10 m)\n",
    "    \"--geometry.max-angle\", \"95\",             # bỏ cảnh báo góc quá nhỏ\n",
    "    \"--junctions.corner-detail\", \"6\",         # bo góc mịn hơn\n",
    "    \"--junctions.join-dist\", \"40\",            # tránh gộp nút quá xa\n",
    "    \"--connections.guess\", \"true\",            # tự tạo connection thiếu\n",
    "    \"--keep-edges.min-speed\", \"5\",            # giữ đường nhỏ ≥ 5 m/s\n",
    "    \"-o\", CLEANED_NET_XML_3                   # đầu ra: mạng sạch cuối\n",
    "]\n",
    "\n",
    "with open(NETCONVERT_LOG_3, \"w\") as f:\n",
    "    print(\"Running NETCONVERT Step 3 (geometry & connections cleanup)...\")\n",
    "    subprocess.run(NETCONVERT_CMD_3,\n",
    "                   stdout=f,\n",
    "                   stderr=subprocess.STDOUT,\n",
    "                   check=True)\n",
    "    print(f\"[DONE] Cleaned Network written to {CLEANED_NET_XML_3}\"\n",
    "          f\"\\n[LOG] Output logged in {NETCONVERT_LOG_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTESTATS_CMD = [\n",
    "    \"python\", ROUTESTATS_PY,\n",
    "    ROUTE_ALT_XML,\n",
    "    # \"-n\", CLEANED_NET_XML_2,\n",
    "    \"-a\", \"routeLength\",\n",
    "    \"--binwidth\", \"500\",\n",
    "    \"--hist-output\", \"/home/hoai-linh.dao/Works/EVCS/CEREMA-Mini/result/hist.dat\"\n",
    "]\n",
    "subprocess.run(ROUTESTATS_CMD, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TAZ - Way 1 (Using with --ignore-errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_tree = ET.parse(GROUPED_POLY_XML)\n",
    "poly_root = poly_tree.getroot()\n",
    "\n",
    "region_polys = defaultdict(list)\n",
    "for poly in poly_root.findall(\"poly\"):\n",
    "    region = poly.get(\"type\")\n",
    "    shape_str = poly.get(\"shape\")\n",
    "    if region and shape_str:\n",
    "        polygon = parseShape(shape_str)\n",
    "        if polygon is not None:\n",
    "            region_polys[region].append(polygon)\n",
    "\n",
    "region_geoms = {}\n",
    "for region, polys in region_polys.items():\n",
    "    if polys:\n",
    "        try:\n",
    "            region_geoms[region] = unary_union(polys)\n",
    "        except TopologicalError as e:\n",
    "            print(f\"[ERROR] Topology error in bassin {region}: {e}\")\n",
    "\n",
    "print(\"[CHECK] Bounding boxes (per basin):\")\n",
    "for region, geom in region_geoms.items():\n",
    "    print(f\"  {region}: {geom.bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = sumolib.net.readNet(CLEANED_NET_XML_2)\n",
    "\n",
    "# store edges per basin; plus list for those outside any basin (\"hors_amp\")\n",
    "edges_by_region = defaultdict(list)\n",
    "edges_hors      = []\n",
    "\n",
    "for edge in net.getEdges():\n",
    "    # skip technical source/sink edges created by netconvert\n",
    "    if edge.getID().endswith(\"-source\") or edge.getID().endswith(\"-sink\"):\n",
    "        continue\n",
    "\n",
    "    shape = edge.getShape()\n",
    "    if not shape:\n",
    "        continue\n",
    "    mid_pt = shape[len(shape) // 2]\n",
    "    pt = Point(mid_pt[0], mid_pt[1])\n",
    "\n",
    "    assigned = False\n",
    "    for region, geom in region_geoms.items():\n",
    "        if geom.contains(pt):\n",
    "            edges_by_region[region].append(edge)\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        edges_hors.append(edge)\n",
    "\n",
    "# add \"hors_amp\" (outside) group\n",
    "edges_by_region[\"hors_amp\"] = edges_hors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_root = ET.Element(\"tazs\")\n",
    "\n",
    "for region, edges in edges_by_region.items():\n",
    "    rid    = region.lower()\n",
    "    taz_id = TAZ_IDS.get(rid)\n",
    "    if not taz_id:\n",
    "        print(f\"[ERROR] No TAZ ID for {region}, skip\")\n",
    "        continue\n",
    "\n",
    "    geom = region_geoms.get(region)\n",
    "    if geom is None:\n",
    "        B, I = [], edges[:]\n",
    "    else:\n",
    "        B = selectBoundaryEdges(edges, geom, threshold_ratio=0.1)\n",
    "        I = [e for e in edges if e not in B]\n",
    "\n",
    "    if region == \"hors_amp\":\n",
    "        conns = edges[:]\n",
    "    else:\n",
    "        total = len(B) + len(I)\n",
    "        nB    = int(BORDER_RATIO * total)\n",
    "        nI    = total - nB\n",
    "        conns = random.sample(B, min(nB, len(B))) + \\\n",
    "                random.sample(I, min(nI, len(I)))\n",
    "\n",
    "    print(f\"[CHECK] Basin {region}: total={len(edges)} | B={len(B)} | I={len(I)} | selected={len(conns)}\")\n",
    "\n",
    "    cent = geom.centroid if geom is not None else Point(0, 0)\n",
    "    taz  = ET.SubElement(taz_root, \"taz\", id=str(taz_id), x=f\"{cent.x:.2f}\", y=f\"{cent.y:.2f}\")\n",
    "    for e in sorted(conns, key=lambda _e: _e.getID()):\n",
    "        ET.SubElement(taz, \"tazSource\", id=e.getID(), weight=\"1.0\")\n",
    "        ET.SubElement(taz, \"tazSink\",   id=e.getID(), weight=\"1.0\")\n",
    "\n",
    "ET.ElementTree(taz_root).write(TAZ_XML, encoding=\"utf-8\", xml_declaration=True)\n",
    "print(f\"\\n[DONE] TAZ file written to {TAZ_XML}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TAZ - Way 2 (More Roburst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_INNER = 40  \n",
    "SAMPLE_CROSS = 40  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP‑1  Build basin geometries\n",
    "print(\"Running Step 1 - Reading bassins ...\")\n",
    "region_geoms = defaultdict(list)\n",
    "for p in ET.parse(GROUPED_POLY_XML).getroot().findall('poly'):\n",
    "    reg = p.get('type')\n",
    "    geom = parseShape(p.get('shape', ''))\n",
    "    if reg and geom:\n",
    "        region_geoms[reg].append(geom)\n",
    "for reg, polys in region_geoms.items():\n",
    "    region_geoms[reg] = unary_union(polys) if len(polys) > 1 else polys[0]\n",
    "print(f\"[DONE] Step 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP‑2  Scan network & assign edges to basins\n",
    "print(\"Running Step 2 - Scan network & assign edges to basins ...\")\n",
    "NET = sumolib.net.readNet(CLEANED_NET_XML_2)\n",
    "\n",
    "edges_by_region = defaultdict(list)\n",
    "outside = []\n",
    "\n",
    "for e in NET.getEdges():\n",
    "    if not isValidEdge(e):\n",
    "        continue\n",
    "    mid = Point(e.getShape()[len(e.getShape())//2])\n",
    "    placed = False\n",
    "    for reg, geom in region_geoms.items():\n",
    "        if geom.contains(mid):\n",
    "            edges_by_region[reg].append(e)\n",
    "            placed = True\n",
    "            break\n",
    "    if not placed:\n",
    "        outside.append(e)\n",
    "\n",
    "edges_by_region['hors_amp'] = outside\n",
    "\n",
    "# ── Inner‑basin connectivity \n",
    "print(\"--- Step 2a - Inner‑basin connectivity filter ...\")\n",
    "for reg, pool in list(edges_by_region.items()):\n",
    "    kept = filterReachable(pool, NET, SAMPLE_INNER)\n",
    "    if len(kept) < len(pool):\n",
    "        print(f\"  – {reg}: removed {len(pool)-len(kept)} isolated edges\")\n",
    "    edges_by_region[reg] = kept\n",
    "\n",
    "# ── Cross‑basin connectivity \n",
    "print(\"--- Step 2b - Cross‑basin connectivity filter ...\")\n",
    "for reg_from, pool_from in list(edges_by_region.items()):\n",
    "    others = [e for r, p in edges_by_region.items() if r != reg_from for e in p]\n",
    "    if not pool_from or not others:\n",
    "        continue\n",
    "    keep = []\n",
    "    for e in pool_from:\n",
    "        tgt_sample = random.sample(others, min(SAMPLE_CROSS, len(others)))\n",
    "        ok_out = any(reachable(e, t, NET) for t in tgt_sample)\n",
    "        ok_in  = any(reachable(t, e, NET) for t in tgt_sample)\n",
    "        if ok_out and ok_in:\n",
    "            keep.append(e)\n",
    "    removed = len(pool_from) - len(keep)\n",
    "    if removed:\n",
    "        print(f\"  – {reg_from}: removed {removed} edges not reachable cross‑basin\")\n",
    "    edges_by_region[reg_from] = keep\n",
    "print(f\"[DONE] Step 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP‑3  Write TAZ\n",
    "print(\"Running Step 3 - Write TAZ ...\")\n",
    "root = ET.Element('tazs')\n",
    "for reg, pool in edges_by_region.items():\n",
    "    if not pool:\n",
    "        continue\n",
    "    tid = TAZ_IDS.get(reg.lower())\n",
    "    if tid is None:\n",
    "        print(f\"  ! no TAZ id for basin {reg}; skip\")\n",
    "        continue\n",
    "    geom = region_geoms.get(reg)\n",
    "    B = selectBoundaryEdges(pool, geom)\n",
    "    I = [e for e in pool if e not in B]\n",
    "    nb = int(BORDER_RATIO*len(pool))\n",
    "    ni = len(pool)-nb\n",
    "    chosen = random.sample(B, min(nb, len(B))) + random.sample(I, min(ni, len(I)))\n",
    "\n",
    "    c = geom.centroid if geom else Point(0,0)\n",
    "    taz = ET.SubElement(root, 'taz', id=str(tid), x=f\"{c.x:.2f}\", y=f\"{c.y:.2f}\")\n",
    "    for e in sorted(chosen, key=lambda x:x.getID()):\n",
    "        ET.SubElement(taz,'tazSource', id=e.getID(), weight='1.0')\n",
    "        ET.SubElement(taz,'tazSink',   id=e.getID(), weight='1.0')\n",
    "\n",
    "ET.ElementTree(root).write(TAZ_XML, encoding='utf-8', xml_declaration=True)\n",
    "print(f\"[DONE] {TAZ_XML} ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_files = generateOds(\n",
    "    MAIN_FLOW_CSV,\n",
    "    ODS_DIR,\n",
    "    TAZ_IDS,\n",
    "    real_origin=\"marseille\",\n",
    "    exclude_cols={\"total\",\"intra\"},\n",
    "    trips_ratio=TRIPS_RATIO_4,\n",
    "    scale_in=INCOMING_RATIO,\n",
    "    scale_out=OUTGOING_RATIO\n",
    ")\n",
    "\n",
    "\n",
    "for hour, path in matrix_files:\n",
    "    size = os.path.getsize(path)\n",
    "    print(f\"[DONE] OD matrix hour {hour}: {path} ({size} bytes)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vtypes Distribution (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignProbabilitiesToVtypes(\n",
    "    vtypes_xml=ORIG_VTYPES_XML,\n",
    "    dist_id=\"vehDist\",\n",
    "    ev_brands=EV_BRANDS,\n",
    "    ev_ratio=0.2,\n",
    "    output_xml=VTYPES_DIST_XML\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Trips from Ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Call od2trips for all ...\")\n",
    "trips_files = od2tripsForAll(TAZ_XML, TRIPS_DIR, ODS_DIR, DIST_ID)\n",
    "print()\n",
    "print(\"[DONE] Finished 24 Trips based on hours.\")\n",
    "\n",
    "mergeTrips(TRIPS_DIR, ALL_TRIPS_XML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUAROUTER_ADDS = \",\".join([str(VTYPES_DIST_XML), str(TAZ_XML)])\n",
    "DUAROUTER_CMD = [\n",
    "    \"duarouter\",\n",
    "    \"-n\", CLEANED_NET_XML_2,            \n",
    "    \"-r\", ALL_TRIPS_XML,            \n",
    "    # \"-a\", DUAROUTER_ADDS,\n",
    "    \"-a\", VTYPES_DIST_XML,\n",
    "    # \"--keep-vtype-distributions\",\n",
    "    # \"--with-taz\",\n",
    "    # \"--repair\",                      \n",
    "    # \"--remove-loops\",               \n",
    "    \"--randomize-flows\",         \n",
    "    \"-o\", ROUTE_XML,    \n",
    "    \"--log\", DUAROUTER_LOG,\n",
    "    \"--exit-times\",\n",
    "    \"--named-routes\",\n",
    "    \"--route-length\",\n",
    "    \"--write-costs\"\n",
    "]\n",
    "\n",
    "print(\"Running DUAROUTER Step ...\")\n",
    "subprocess.run(DUAROUTER_CMD, check=True)\n",
    "print(f\"[DONE] Routes written in {ROUTE_XML}\\n[LOG] Output logged in {DUAROUTER_LOG}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ReRouter (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REROUTING_CMD = [\n",
    "    \"python\", REROUTING_PY,\n",
    "    \"-n\", CLEANED_NET_XML_2,\n",
    "    \"-o\", REROUTER_XML,\n",
    "    \"--vclass\", \"passenger\",\n",
    "]\n",
    "\n",
    "with open(REROUTING_LOG, \"w\") as f:\n",
    "    print(\"Running REROUTING Step ...\")\n",
    "    subprocess.run(REROUTING_CMD, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Rerouter file is created in {REROUTER_XML}\\n[LOG] Output logged in {REROUTING_LOG}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Sumo-Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateSumoCfg(\n",
    "    cfg_path=SUMOCFG_XML,\n",
    "    output_path=SUMOCFG_XML,\n",
    "    replacements=PATH_REPLACEMENTS\n",
    ")\n",
    "print(f\"[DONE] New Sumo-Configuration in {ROUTE_XML} with all right paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION_CMD = [\n",
    "    \"sumo\",             \n",
    "    \"-c\", SUMOCFG_XML,\n",
    "    \"--no-step-log\",      \n",
    "    \"--duration-log.statistics\",\n",
    "    \"--xml-validation\", \"never\"  \n",
    "]\n",
    "\n",
    "with open(SIMULATION_LOG, \"w\") as f:\n",
    "    print(\"Running SUMO simulation ...\")\n",
    "    subprocess.run(SIMULATION_CMD, stdout=f, stderr=subprocess.STDOUT, check=True)\n",
    "    print(f\"[DONE] Simulation outputs are created in {SIMULATION_DIR}\\n[LOG] Output logged in {SIMULATION_LOG}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VISUALIZATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departure times versus arrival times\n",
    "PLOT_CMD_1 = [\n",
    "    \"python\", PLOTXMLATTRIBUTES_PY,\n",
    "    VEHROUTES_XML,                 \n",
    "    \"-x\", \"depart\",      \n",
    "    \"-y\", \"arrival\",             \n",
    "    \"-o\", PLOT_1_PNG,\n",
    "    \"--scatterplot\"\n",
    "]\n",
    "\n",
    "subprocess.run(PLOT_CMD_1, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All trajectories over time 1\n",
    "PLOT_CMD_2 = [\n",
    "    \"python\", PLOTXMLATTRIBUTES_PY,\n",
    "    TRACE_XML,                 \n",
    "    \"-x\", \"x\",     \n",
    "    \"-y\", \"y\",             \n",
    "    \"-o\", PLOT_2_PNG,\n",
    "    \"--scatterplot\"\n",
    "]\n",
    "\n",
    "subprocess.run(PLOT_CMD_2, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple timelines from summary-output\n",
    "PLOT_CMD_3 = [\n",
    "    \"python\", PLOTXMLATTRIBUTES_PY,\n",
    "    SUMMARY_XML,\n",
    "    \"-x\", \"time\",\n",
    "    \"-y\", \"running,halting\",\n",
    "    \"-o\", PLOT_3_PNG,\n",
    "    \"--legend\"\n",
    "]\n",
    "subprocess.run(PLOT_CMD_3, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depart delay over time from TripInfo data\n",
    "PLOT_CMD_4 = [\n",
    "    \"python\", PLOTXMLATTRIBUTES_PY,\n",
    "    TRIPINFO_XML,\n",
    "    \"-x\", \"depart\",\n",
    "    \"-y\", \"departDelay\",\n",
    "    \"--xlabel\", \"depart time [s]\",\n",
    "    \"--ylabel\", \"depart delay [s]\",\n",
    "    \"--ylim\", \"0,40\",\n",
    "    \"--xticks\", \"0,1200,200,10\",\n",
    "    \"--yticks\", \"0,40,5,10\",\n",
    "    \"--xgrid\", \"--ygrid\",\n",
    "    \"--title\", \"depart delay over depart time\",\n",
    "    \"--titlesize\", \"16\",\n",
    "    \"-o\", PLOT_4_PNG\n",
    "]\n",
    "subprocess.run(PLOT_CMD_4, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIED_VEH_ID = \"carDist1\"\n",
    "# Selected trajectories over time\n",
    "PLOT_CMD_5 = [\n",
    "    \"python\", PLOTXMLATTRIBUTES_PY,\n",
    "    TRACE_XML,\n",
    "    \"-x\", \"x\",\n",
    "    \"-y\", \"y\",\n",
    "    \"-i\", \"id\",\n",
    "    \"--filter-ids\", QUERIED_VEH_ID\n",
    "    \"--scatterplot\",\n",
    "    \"--legend\",\n",
    "    \"-o\", PLOT_5_PNG\n",
    "]\n",
    "subprocess.run(PLOT_CMD_4, check=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
